{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-17T14:41:57.057208Z","iopub.execute_input":"2022-05-17T14:41:57.057568Z","iopub.status.idle":"2022-05-17T14:41:57.077308Z","shell.execute_reply.started":"2022-05-17T14:41:57.057475Z","shell.execute_reply":"2022-05-17T14:41:57.076294Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#data processing\nimport re, string\nimport emoji\nimport nltk\n\nfrom sklearn import preprocessing\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.model_selection import train_test_split\n\n\n#transformers\nfrom transformers import BertTokenizerFast\nfrom transformers import TFBertModel\nfrom transformers import RobertaTokenizerFast\nfrom transformers import TFRobertaModel\n\n#keras\nimport tensorflow as tf\nfrom tensorflow import keras\n\n\n#metrics\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n#set seed for reproducibility\nseed=42\n\n#set style for plots\nsns.set_style(\"whitegrid\")\nsns.despine()\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlepad=10)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T14:41:57.132015Z","iopub.execute_input":"2022-05-17T14:41:57.132395Z","iopub.status.idle":"2022-05-17T14:41:57.147062Z","shell.execute_reply.started":"2022-05-17T14:41:57.132362Z","shell.execute_reply":"2022-05-17T14:41:57.145986Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!pip install transformers\n","metadata":{"execution":{"iopub.status.busy":"2022-05-17T14:41:57.236340Z","iopub.execute_input":"2022-05-17T14:41:57.236812Z","iopub.status.idle":"2022-05-17T14:42:10.089974Z","shell.execute_reply.started":"2022-05-17T14:41:57.236781Z","shell.execute_reply":"2022-05-17T14:42:10.088769Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def conf_matrix(y, y_pred, title):\n    fig, ax =plt.subplots(figsize=(5,5))\n    labels=['Negative', 'Positive']\n    ax=sns.heatmap(confusion_matrix(y, y_pred), annot=True, cmap=\"Blues\", fmt='g', cbar=False, annot_kws={\"size\":25})\n    plt.title(title, fontsize=20)\n    ax.xaxis.set_ticklabels(labels, fontsize=17) \n    ax.yaxis.set_ticklabels(labels, fontsize=17)\n    ax.set_ylabel('Test', fontsize=20)\n    ax.set_xlabel('Predicted', fontsize=20)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T14:42:10.093730Z","iopub.execute_input":"2022-05-17T14:42:10.094375Z","iopub.status.idle":"2022-05-17T14:42:10.103633Z","shell.execute_reply.started":"2022-05-17T14:42:10.094325Z","shell.execute_reply":"2022-05-17T14:42:10.102524Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('../input/result123/result.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-17T14:47:34.018570Z","iopub.execute_input":"2022-05-17T14:47:34.018935Z","iopub.status.idle":"2022-05-17T14:47:34.213083Z","shell.execute_reply.started":"2022-05-17T14:47:34.018896Z","shell.execute_reply":"2022-05-17T14:47:34.212149Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T14:47:37.728469Z","iopub.execute_input":"2022-05-17T14:47:37.729108Z","iopub.status.idle":"2022-05-17T14:47:37.752817Z","shell.execute_reply.started":"2022-05-17T14:47:37.729072Z","shell.execute_reply":"2022-05-17T14:47:37.751933Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df.drop('timestamp', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T14:42:10.597083Z","iopub.status.idle":"2022-05-17T14:42:10.598052Z","shell.execute_reply.started":"2022-05-17T14:42:10.597749Z","shell.execute_reply":"2022-05-17T14:42:10.597780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop('num', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T14:42:10.599595Z","iopub.status.idle":"2022-05-17T14:42:10.600813Z","shell.execute_reply.started":"2022-05-17T14:42:10.600450Z","shell.execute_reply":"2022-05-17T14:42:10.600504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##CUSTOM DEFINED FUNCTIONS TO CLEAN THE TWEETS\n\n#Clean emojis from text\ndef strip_emoji(text):\n    return re.sub(emoji.get_emoji_regexp(), r\"\", text) #remove emoji\n\n#Remove punctuations, links, mentions and \\r\\n new line characters\ndef strip_all_entities(text): \n    text = text.replace('\\r', '').replace('\\n', ' ').replace('\\n', ' ').lower() #remove \\n and \\r and lowercase\n    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text) #remove links and mentions\n    text = re.sub(r'[^\\x00-\\x7f]',r'', text) #remove non utf8/ascii characters such as '\\x9a\\x91\\x97\\x9a\\x97'\n    banned_list= string.punctuation + 'Ã'+'±'+'ã'+'¼'+'â'+'»'+'§'\n    table = str.maketrans('', '', banned_list)\n    text = text.translate(table)\n    return text\n\n#clean hashtags at the end of the sentence, and keep those in the middle of the sentence by removing just the # symbol\ndef clean_hashtags(tweet):\n    new_tweet = \" \".join(word.strip() for word in re.split('#(?!(?:hashtag)\\b)[\\w-]+(?=(?:\\s+#[\\w-]+)*\\s*$)', tweet)) #remove last hashtags\n    new_tweet2 = \" \".join(word.strip() for word in re.split('#|_', new_tweet)) #remove hashtags symbol from words in the middle of the sentence\n    return new_tweet2\n\n#Filter special characters such as & and $ present in some words\ndef filter_chars(a):\n    sent = []\n    for word in a.split(' '):\n        if ('$' in word) | ('&' in word):\n            sent.append('')\n        else:\n            sent.append(word)\n    return ' '.join(sent)\n\ndef remove_mult_spaces(text): # remove multiple spaces\n    return re.sub(\"\\s\\s+\" , \" \", text)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T14:52:40.281389Z","iopub.execute_input":"2022-05-17T14:52:40.281822Z","iopub.status.idle":"2022-05-17T14:52:40.299225Z","shell.execute_reply.started":"2022-05-17T14:52:40.281779Z","shell.execute_reply":"2022-05-17T14:52:40.298302Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"texts_new = []\nfor t in df.text:\n    texts_new.append(remove_mult_spaces(filter_chars(clean_hashtags(strip_all_entities(strip_emoji(t))))))","metadata":{"execution":{"iopub.status.busy":"2022-05-17T14:52:40.372415Z","iopub.execute_input":"2022-05-17T14:52:40.375782Z","iopub.status.idle":"2022-05-17T14:53:13.447427Z","shell.execute_reply.started":"2022-05-17T14:52:40.375719Z","shell.execute_reply":"2022-05-17T14:53:13.446352Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"df['text_clean'] = texts_new\n","metadata":{"execution":{"iopub.status.busy":"2022-05-17T14:53:13.453630Z","iopub.execute_input":"2022-05-17T14:53:13.455957Z","iopub.status.idle":"2022-05-17T14:53:13.467767Z","shell.execute_reply.started":"2022-05-17T14:53:13.455913Z","shell.execute_reply":"2022-05-17T14:53:13.466870Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df.drop('text', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T14:53:13.473594Z","iopub.execute_input":"2022-05-17T14:53:13.476634Z","iopub.status.idle":"2022-05-17T14:53:13.497840Z","shell.execute_reply.started":"2022-05-17T14:53:13.476586Z","shell.execute_reply":"2022-05-17T14:53:13.496926Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nstop = stopwords.words('english')\ndf['text_clean'] = df['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))","metadata":{"execution":{"iopub.status.busy":"2022-05-17T14:53:13.503358Z","iopub.execute_input":"2022-05-17T14:53:13.505849Z","iopub.status.idle":"2022-05-17T14:53:14.655568Z","shell.execute_reply.started":"2022-05-17T14:53:13.505806Z","shell.execute_reply":"2022-05-17T14:53:14.654601Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n","metadata":{"execution":{"iopub.status.busy":"2022-05-17T14:53:14.656899Z","iopub.execute_input":"2022-05-17T14:53:14.657224Z","iopub.status.idle":"2022-05-17T14:53:20.440464Z","shell.execute_reply.started":"2022-05-17T14:53:14.657153Z","shell.execute_reply":"2022-05-17T14:53:20.439528Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"token_lens = []\n\nfor txt in df['text_clean'].values:\n    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n    token_lens.append(len(tokens))\n    \nmax_len=np.max(token_lens)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T14:53:20.442332Z","iopub.execute_input":"2022-05-17T14:53:20.442685Z","iopub.status.idle":"2022-05-17T14:53:25.815680Z","shell.execute_reply.started":"2022-05-17T14:53:20.442641Z","shell.execute_reply":"2022-05-17T14:53:25.814707Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"print(f\"MAX TOKENIZED SENTENCE LENGTH: {max_len}\")\n","metadata":{"execution":{"iopub.status.busy":"2022-05-17T14:53:25.817410Z","iopub.execute_input":"2022-05-17T14:53:25.817766Z","iopub.status.idle":"2022-05-17T14:53:25.823834Z","shell.execute_reply.started":"2022-05-17T14:53:25.817722Z","shell.execute_reply":"2022-05-17T14:53:25.822833Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"token_lens = []\n\nfor i,txt in enumerate(df['text_clean'].values):\n    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n    token_lens.append(len(tokens))\n    if len(tokens)>40:\n        print(f\"INDEX: {i}, TEXT: {txt}\")  ","metadata":{"execution":{"iopub.status.busy":"2022-05-17T14:53:25.825399Z","iopub.execute_input":"2022-05-17T14:53:25.825955Z","iopub.status.idle":"2022-05-17T14:53:30.907244Z","shell.execute_reply.started":"2022-05-17T14:53:25.825910Z","shell.execute_reply":"2022-05-17T14:53:30.906345Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"X = df['text_clean'].values\ny = df['label'].values","metadata":{"execution":{"iopub.status.busy":"2022-05-17T14:53:30.908599Z","iopub.execute_input":"2022-05-17T14:53:30.908890Z","iopub.status.idle":"2022-05-17T14:53:30.914845Z","shell.execute_reply.started":"2022-05-17T14:53:30.908849Z","shell.execute_reply":"2022-05-17T14:53:30.913564Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T14:53:30.920282Z","iopub.execute_input":"2022-05-17T14:53:30.920587Z","iopub.status.idle":"2022-05-17T14:53:30.950387Z","shell.execute_reply.started":"2022-05-17T14:53:30.920548Z","shell.execute_reply":"2022-05-17T14:53:30.949399Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.25, random_state=seed) ","metadata":{"execution":{"iopub.status.busy":"2022-05-17T14:53:30.952569Z","iopub.execute_input":"2022-05-17T14:53:30.952963Z","iopub.status.idle":"2022-05-17T14:53:30.963991Z","shell.execute_reply.started":"2022-05-17T14:53:30.952920Z","shell.execute_reply":"2022-05-17T14:53:30.962990Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"MAX_LEN=128\ndef tokenize(data,max_len=MAX_LEN) :\n    input_ids = []\n    attention_masks = []\n    for i in range(len(data)):\n        encoded = tokenizer.encode_plus(\n            data[i],\n            add_special_tokens=True,\n            max_length=MAX_LEN,\n            padding='max_length',\n            return_attention_mask=True\n        )\n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n    return np.array(input_ids),np.array(attention_masks)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T14:53:30.965968Z","iopub.execute_input":"2022-05-17T14:53:30.966594Z","iopub.status.idle":"2022-05-17T14:53:30.975668Z","shell.execute_reply.started":"2022-05-17T14:53:30.966547Z","shell.execute_reply":"2022-05-17T14:53:30.974648Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"train_input_ids, train_attention_masks = tokenize(X_train, MAX_LEN)\nval_input_ids, val_attention_masks = tokenize(X_valid, MAX_LEN)\ntest_input_ids, test_attention_masks = tokenize(X_test, MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T14:53:30.977176Z","iopub.execute_input":"2022-05-17T14:53:30.977813Z","iopub.status.idle":"2022-05-17T14:53:38.639524Z","shell.execute_reply.started":"2022-05-17T14:53:30.977768Z","shell.execute_reply":"2022-05-17T14:53:38.638539Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n","metadata":{"execution":{"iopub.status.busy":"2022-05-17T14:53:38.641121Z","iopub.execute_input":"2022-05-17T14:53:38.641452Z","iopub.status.idle":"2022-05-17T14:54:06.857864Z","shell.execute_reply.started":"2022-05-17T14:53:38.641411Z","shell.execute_reply":"2022-05-17T14:54:06.856756Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def create_model(bert_model, max_len=MAX_LEN):\n    \n    ##params###\n    opt = tf.keras.optimizers.Adam(learning_rate=3e-5, decay=1e-7)\n    loss = tf.keras.losses.BinaryCrossentropy()\n    accuracy = tf.keras.metrics.BinaryAccuracy()\n\n\n    input_ids = tf.keras.Input(shape=(max_len,),dtype='int32')\n    \n    attention_masks = tf.keras.Input(shape=(max_len,),dtype='int32')\n    \n    embeddings = bert_model([input_ids,attention_masks])[1]\n    \n    output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(embeddings)\n    \n    model = tf.keras.models.Model(inputs = [input_ids,attention_masks], outputs = output)\n    \n    model.compile(opt, loss=loss, metrics=accuracy)\n    \n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-17T14:54:06.860052Z","iopub.execute_input":"2022-05-17T14:54:06.860765Z","iopub.status.idle":"2022-05-17T14:54:06.870711Z","shell.execute_reply.started":"2022-05-17T14:54:06.860695Z","shell.execute_reply":"2022-05-17T14:54:06.869765Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"model = create_model(bert_model, MAX_LEN)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T14:54:06.872524Z","iopub.execute_input":"2022-05-17T14:54:06.873051Z","iopub.status.idle":"2022-05-17T14:54:15.131031Z","shell.execute_reply.started":"2022-05-17T14:54:06.873003Z","shell.execute_reply":"2022-05-17T14:54:15.129972Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"history_bert = model.fit([train_input_ids,train_attention_masks], y_train, validation_data=([val_input_ids,val_attention_masks], y_valid), epochs=5, batch_size=32)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-17T15:08:27.223609Z","iopub.execute_input":"2022-05-17T15:08:27.223979Z","iopub.status.idle":"2022-05-17T15:36:49.286987Z","shell.execute_reply.started":"2022-05-17T15:08:27.223937Z","shell.execute_reply":"2022-05-17T15:36:49.285957Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"result_bert = model.predict([test_input_ids,test_attention_masks])\n","metadata":{"execution":{"iopub.status.busy":"2022-05-17T15:37:27.629268Z","iopub.execute_input":"2022-05-17T15:37:27.629533Z","iopub.status.idle":"2022-05-17T15:38:02.652480Z","shell.execute_reply.started":"2022-05-17T15:37:27.629469Z","shell.execute_reply":"2022-05-17T15:38:02.651548Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"y_pred_bert =  np.zeros_like(result_bert)\ny_pred_bert[np.arange(len(y_pred_bert)), result_bert.argmax(1)] = 1","metadata":{"execution":{"iopub.status.busy":"2022-05-17T15:40:23.688625Z","iopub.execute_input":"2022-05-17T15:40:23.688925Z","iopub.status.idle":"2022-05-17T15:40:23.695477Z","shell.execute_reply.started":"2022-05-17T15:40:23.688895Z","shell.execute_reply":"2022-05-17T15:40:23.694346Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"conf_matrix(y_test, y_pred_,'BERT Sentiment Analysis\\nConfusion Matrix')","metadata":{"execution":{"iopub.status.busy":"2022-05-17T15:08:25.602058Z","iopub.status.idle":"2022-05-17T15:08:25.602878Z","shell.execute_reply.started":"2022-05-17T15:08:25.602525Z","shell.execute_reply":"2022-05-17T15:08:25.602578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('F1 Score:\\n\\n',f1_score(y_test,result_bert.round()))","metadata":{"execution":{"iopub.status.busy":"2022-05-17T15:40:29.719879Z","iopub.execute_input":"2022-05-17T15:40:29.720743Z","iopub.status.idle":"2022-05-17T15:40:29.736185Z","shell.execute_reply.started":"2022-05-17T15:40:29.720710Z","shell.execute_reply":"2022-05-17T15:40:29.734974Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}